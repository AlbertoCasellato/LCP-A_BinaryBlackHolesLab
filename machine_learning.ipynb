{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a60284-c7bc-4409-9a55-4416cfba8f04",
   "metadata": {},
   "source": [
    "# Analysis of the sistem\n",
    "The binary system has two different kind of evolutions: via mass transfer or common envelope. In the first case the two stars of the system exchange mass directly with each other; in the latter case they exchange mass through a common envelope. The aim of the following analysis is to infer the wheight of each parameter in the kind of evolution. The results are supported both by different kind of machine learning algorithms and different approaches. Then with the heaviest ones new features will be derived combining the original ones among those which play the major role in the evolution, so new parameters with physical meaning are obtained. After this first step the analysis is performed again on the derived features with the aim of finding wheights also for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439561f8-ed51-4de0-a942-928ec299ec23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Basic libraries, functions, paths and parameters\n",
    "Here are specified all the libraries used in the project, generic functions used for managing import and export of files, directories used and global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1796ed-554d-47b1-b236-2b00e47efdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import os                                              # for reading files\n",
    "import numpy as np                                     # utilites\n",
    "import pandas as pd                                    # dataframes\n",
    "import matplotlib.pyplot as plt                        # plotting\n",
    "#\n",
    "import pickle                                          # For saving and reading python3 objects\n",
    "#\n",
    "from sklearn.model_selection import train_test_split   # For splitting dataframe in (x, y)_train and (x, y)_test\n",
    "from sklearn.preprocessing import StandardScaler       # For re-scale data (subtract average, divide by standard deviation)\n",
    "from sklearn.ensemble import RandomForestClassifier    # The Random Forest Model\n",
    "from sklearn.metrics import confusion_matrix           # To evaluate the predict power of the model\n",
    "from sklearn.svm import SVC                            # The Support Vector Machines Model\n",
    "#\n",
    "#\n",
    "# basic functions\n",
    "SAVE   = lambda data, dir, name: pickle.dump(data, open(dir + name + \".pkl\", \"wb\"))   # save file\n",
    "LOAD   = lambda file:            pickle.load(open(file + \".pkl\", \"rb\"))               # read file\n",
    "CREATE = lambda data, model: data.data if data.is_loaded else model                   # loads models properly\n",
    "#\n",
    "# read if file exists\n",
    "def TRY_LOAD(file):\n",
    "    try:\n",
    "        data = LOAD(file)\n",
    "        return data\n",
    "    except:\n",
    "        return 0\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "#####\n",
    "#\n",
    "#\n",
    "# making confusion matrix\n",
    "def make_confusion_matrix(y_test, y_pred):\n",
    "    conf_matrx = pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                              columns = [\"Predicted_True\", \"Predicted_False\"],\n",
    "                              index   = [\"Actual_True\", \"Actual_False\"])\n",
    "    true_positive  = conf_matrx.loc[\"Actual_True\",   \"Predicted_True\"]\n",
    "    true_negative  = conf_matrx.loc[\"Actual_False\", \"Predicted_False\"]\n",
    "    accuracy_score = (true_positive + true_negative) / conf_matrx.sum().sum()\n",
    "    print(\"confusion_matrix: \\n\\n\", conf_matrx)\n",
    "    print(\"\\naccuracy_score: \", accuracy_score, \"\\n\\n\\n\")\n",
    "    return accuracy_score\n",
    "    #\n",
    "#####\n",
    "#\n",
    "# making plot\n",
    "def making_plot(columns, values, title, xlabel, ylabel):\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.bar(columns, values, color = 'skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    #\n",
    "#####\n",
    "#\n",
    "#\n",
    "# basic names\n",
    "dataset_version = {\"ORIGIN\" : \"original_features\",                                         # original features\n",
    "                   \"DER_1\"  : \"der_features_hypothesis_1\",                                 # derived features: ver. 1\n",
    "                   \"DER_2\"  : \"der_features_hypothesis_2\"}                                 # derived features: ver. 2\n",
    "#\n",
    "dir             = {\"RF_partial\"         : \"a_random_forest/a_partial_dataset/\",\n",
    "                   \"RF_all\"             : \"a_random_forest/b_all_dataset/\",\n",
    "                   \"RF_all_th_analysis\" : \"a_random_forest/b_all_dataset/th_analysis/\",\n",
    "                   \"RF_all_impurities\"  : \"a_random_forest/b_all_dataset/gini_impurities/\",\n",
    "                   \"SVM_partial\"        : \"b_SVM/a_partial_dataset/\",\n",
    "                   \"SVM_all\"            : \"b_SVM/b_all_dataset/\"}\n",
    "#\n",
    "model_name      = {\"RF\"    : \"random_forest\",             # it's about the random forest (RF) model\n",
    "                  \"SVM\"    : \"support_vector_machines\",   # it's about support vector machines (SVM) model\n",
    "                  \"th_nls\" : \"th_analysis\",               # it's about a threshold optimization algorithm for binary classification tasks\n",
    "                  \"gini\"   : \"gini_impurities\"}           # it's about evaluation of Gini impurities\n",
    "#\n",
    "# basic parameters of the physical system\n",
    "ALPHA           = [0.5, 1, 3, 5]\n",
    "METAL           = [0.0002, 0.0004, 0.0008, 0.0012, 0.0016, 0.002, 0.004, 0.006, 0.008, 0.012, 0.016, 0.02]\n",
    "#\n",
    "#\n",
    "# global parameters of the script\n",
    "RS_choice   = 42      # random state choice - for replicability\n",
    "debugging   = False   # for debugging mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb712ae-3bc8-43e4-86bd-9e03d5feb6a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data reader\n",
    "\"reading_all_datasets\" is the callable object used to load properly the whole dataset; column names are changed in order to improve readability and the derived dataset is build upon the original one. The specific combination of features, chosen among valuable combinations with physical meaning, is justified by the following analysis. Derived features which play major role in the behaviour of the system are, in the order:\n",
    "1. the ratio between the mass of the second star and the mass of the first one;\n",
    "2. the sum of the masses of the two stars;\n",
    "3. the mass loss by the system expressed as the difference between the total mass of the stars and the total mass of black holes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7b495-188b-4b3f-92c9-33910bcf8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_all_datasets():\n",
    "    # reading original features\n",
    "    def make_df(dir, file):\n",
    "        df     = pd.DataFrame()\n",
    "        df_tmp = pd.read_csv(\"data/\" + dir + \"/\" + file, delimiter = \" \", skiprows = 2)\n",
    "        #\n",
    "        df[\"ALPHA\"] = 0                              # associated with the efficiency of common envelope\n",
    "        df[\"METAL\"] = 0                              # stellar metallicity at which the black holes where produced\n",
    "        df[\"MST1\"]  = df_tmp[\"col.1:m1ZAMS/Msun\"]    # ZAMS mass of the primary member of the binary system (BS) [Msun]\n",
    "        df[\"MST2\"]  = df_tmp[\"col.2:m2ZAMS/Msun\"]    # ZAMS mass of the secondary member of the BS [Msun]\n",
    "        df[\"MBH1\"]  = df_tmp[\"col.3:m1rem/Msun\"]     # mass of the black hole (BH) from the primary member [Msun]\n",
    "        df[\"MBH2\"]  = df_tmp[\"col.4:m2rem/Msun\"]     # mass of the BH from the secondary member [Msun]\n",
    "        df[\"TIME\"]  = df_tmp[\"col.6:delay_time/Myr\"] # time from the formation of the BS to the merger of the two BH [Myr]\n",
    "        df[\"AXIS\"]  = df_tmp[\"col.7:sma/Rsun\"]       # semi-major axis of the BS at the formation of the second-born BH [Rsun] \n",
    "        df[\"ECCE\"]  = df_tmp[\"col.8:ecc\"]            # orbital eccentricity of the BS at the formation of the second-born BH\n",
    "        df[\"ISCE\"]  = df_tmp[\"col.21:CE\"]            # True: the BS undergoes a common envelope. False: the BS goes via stable mass transfer\n",
    "        df[\"ALPHA\"] = np.float64(dir[1:])\n",
    "        df[\"METAL\"] = np.float64(file.split(\"_\")[-1].split(\".txt\")[0])\n",
    "        del df_tmp\n",
    "        return df\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    # making derived features [HYPOTHESIS 1]     # it's the chosen hypothesis\n",
    "    def make_der_df(df):\n",
    "        df_der = pd.DataFrame()\n",
    "        df_der[\"ALPHA\"]           = df[\"ALPHA\"]\n",
    "        df_der[\"METAL\"]           = df[\"METAL\"]\n",
    "        df_der[\"MS_TOT\"]          = df[\"MST2\"] + df[\"MST1\"]                                # MST2+MST1\n",
    "        df_der[\"MS_RATIO\"]        = df[\"MST2\"] / df[\"MST1\"]                                # MST2/MST1\n",
    "        df_der[\"M_LOSS\"]          = 1 - ((df[\"MBH2\"] + df[\"MBH1\"]) / df_der[\"MS_TOT\"])     # 1-[(MBH2+MBH1)/(MST2+MST1)]\n",
    "        df_der[\"M_RATIO_LOSS\"]    = 1 - ((df[\"MBH2\"] / df[\"MBH1\"]) / df_der[\"MS_RATIO\"])   # 1-[(MBH2/MBH1)/(MST2/MST1)]\n",
    "        df_der[\"INITIAL_DENSITY\"] = df_der[\"MS_TOT\"] / (df[\"AXIS\"] ** 2)                   # (MST2+MST1)/(AXIS^2)\n",
    "        df_der[\"ISCE\"]            = df[\"ISCE\"]\n",
    "        return df_der\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    # reading original features\n",
    "    df = [make_df(dir, file) for dir in os.listdir(\"data\") for file in os.listdir(\"data/\" + dir)]\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    #\n",
    "    # making derived features\n",
    "    df_der = make_der_df(df)\n",
    "    #\n",
    "    #\n",
    "    # columns\n",
    "    # [ORIGINAL FEATURES]\n",
    "    original_columns   = df.columns[:-1]\n",
    "    #\n",
    "    # [DERIVED FEATURES - HYPOTHESIS 1]\n",
    "    der_hypo_1_columns = df_der.columns[:-1]\n",
    "    #\n",
    "    # [DERIVED FEATURES - HYPOTHESIS 2]        # it's a rejected hypothesis\n",
    "    der_hypo_2_columns = [\"ALPHA\",             # as hyp 1\n",
    "                          \"METAL\",             # as hyp 1\n",
    "                          \"MS_TOT\",            # as hyp 1\n",
    "                          \"MS_RATIO\",          # as hyp 1\n",
    "                          \"MS_RATIO_INV\",      # MS_RATIO ** (-1)\n",
    "                          \"MBH_RATIO\",         # MBH2 / MBH1\n",
    "                          \"M_LOSS\",            # as hyp 1\n",
    "                          \"M_RATIO_LOSS\",      # as hyp 1\n",
    "                          \"INITIAL_DENSITY\"]   # as hyp 1\n",
    "    #\n",
    "    #\n",
    "    # original features\n",
    "    X0 = df[original_columns]\n",
    "    Y0 = df[\"ISCE\"]\n",
    "    #\n",
    "    # derived features\n",
    "    X1 = df_der[der_hypo_1_columns]\n",
    "    Y1 = df_der[\"ISCE\"]\n",
    "    #\n",
    "    del df, df_der\n",
    "    return X0, Y0, X1, Y1\n",
    "    #\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db62df3-7be7-44db-94ab-f96ab441c974",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### File manager\n",
    "The following class brings automation of files and directory: the model can be loaded regardless of beeing runned or not before the loading. Each file has names and directory assigned to it. It's implemented also a '.save' method to save models properly, without risks of overwritting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6757470-afd0-4164-b963-49d58e096b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to organize files\n",
    "class DATA(object):\n",
    "    def load(self):\n",
    "        if (not self.is_loaded) and self.if_exist:\n",
    "            self.data      = TRY_LOAD(self.path)\n",
    "            self.is_loaded = True\n",
    "        elif not self.if_exist:\n",
    "            print(\"file '\" + self.path + \"' does not exist\")\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def __init__(self, dataset_ver, dir, model_name = \"\"):\n",
    "        model_name      += model_name if model_name == \"\" else \"_\"\n",
    "        self.dataset_ver = model_name + dataset_ver\n",
    "        self.dir         = dir\n",
    "        self.path        = self.dir + self.dataset_ver\n",
    "        self.tmp         = TRY_LOAD(self.path)\n",
    "        self.data        = None\n",
    "        self.is_loaded   = False\n",
    "        self.if_exist    = not isinstance(self.tmp, int)\n",
    "        del self.tmp\n",
    "        #\n",
    "        self.load()\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def save(self, data):\n",
    "        if not self.if_exist:\n",
    "            SAVE(data, self.dir, self.dataset_ver)\n",
    "            self.if_exist = True\n",
    "        else:\n",
    "            print(\"file '\" + self.path + \"' exists yet\")\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac23a05-6280-4f55-b3c3-d0ff6bfa2445",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Models manager\n",
    "The 'MODEL' class takes care properly of models and dataframes. Each 'MODEL()' instance has its data ('X' and 'Y') and it's possible to specify parameters of the models to run.\n",
    "\n",
    "For the two algorithm of the 'sklearn' library, implemented in the 'RandomForestClassifier' and 'SVC' classes, a training process is performed: the dataframe is splitted in two sub-dataframes using the function 'train_test_split' of the 'sklearn' library. In particular the training set is setted to be the 80% of the original one and the test set is the remaining 20%.\n",
    "\n",
    "The test set is then used to evaluate the 'accuracy_score' performed by the model, simply defined as the sum of both the true positive and true negative, divided by the total guesses.\n",
    "\n",
    "The weight of each feature is extracted used the appropriate attribute of the object. In SVM both the training set and the test set are re-scaled, before performing the algorithm, using the 'StandardScaler()' object; it moves all the data subtracting the average of the set, and dived by the standard deviation of the set. This re-scaling operation is needed in order to prevent bias in the model, which can suffer from the extension of the data. With re-scaling only the information which matters is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a0de1-05eb-4c1d-a740-7c9499e741e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to organize models\n",
    "class MODEL(object):\n",
    "    def __init__(self, model_name, X, Y, all, crit = \"gini\", featrs = \"sqrt\", K = \"linear\", D = 3):\n",
    "        self.model_name     = model_name\n",
    "        self.X              = X\n",
    "        self.columns        = self.X.columns\n",
    "        self.Y              = Y\n",
    "        self.crit           = crit\n",
    "        self.featrs         = featrs\n",
    "        self.K              = K\n",
    "        self.D              = D\n",
    "        self.if_all         = all\n",
    "        self.model          = None\n",
    "        self.accuracy_score = None\n",
    "        self.weights        = None\n",
    "        self.if_runned      = False\n",
    "        if not self.if_all:\n",
    "            x_8, x_2, y_8, y_2 = train_test_split(self.X, self.Y, test_size = 0.2, random_state = RS_choice)\n",
    "        else:\n",
    "            x_8, x_2, y_8, y_2 = None, None, None, None\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = x_8, x_2, y_8, y_2\n",
    "        del x_8, x_2, y_8, y_2\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def run(self):\n",
    "        if not self.if_runned:\n",
    "            if self.if_all:\n",
    "                X  = self.X\n",
    "                Y  = self.Y\n",
    "            else:\n",
    "                X  = self.X_train\n",
    "                Y  = self.Y_train\n",
    "                Xt = self.X_test\n",
    "                Yt = self.Y_test\n",
    "                #\n",
    "            #####\n",
    "            #\n",
    "            #\n",
    "            if   self.model_name == model_name[\"RF\"]:\n",
    "                # feedback\n",
    "                print(\"'\" + model_name[\"RF\"] + \"' chosen\")\n",
    "                #\n",
    "                # Random Forest\n",
    "                model = return_RF_model(self.crit, self.featrs)\n",
    "                #\n",
    "                # running\n",
    "                model.fit(X, Y)\n",
    "                #\n",
    "                # testing\n",
    "                if not self.if_all:\n",
    "                    Yp                  = model.predict(Xt)\n",
    "                    self.accuracy_score = make_confusion_matrix(Yt, Yp)\n",
    "                    #\n",
    "                #####\n",
    "                #\n",
    "                # wheigts saving\n",
    "                self.weights = pd.DataFrame(model.feature_importances_, self.columns).T\n",
    "                #\n",
    "                #\n",
    "            elif self.model_name == model_name[\"SVM\"]:\n",
    "                # feedback\n",
    "                print(\"'\" + model_name[\"SVM\"] + \"' chosen\")\n",
    "                #\n",
    "                # SVM\n",
    "                model = return_SVM_model(self.K, self.D)\n",
    "                #\n",
    "                # scaling data\n",
    "                scaler = StandardScaler()\n",
    "                X      = scaler.fit_transform(X)\n",
    "                #\n",
    "                # running\n",
    "                model.fit(X, Y)\n",
    "                #\n",
    "                # testing\n",
    "                if not self.if_all:\n",
    "                    Yp                  = model.predict(scaler.transform(Xt))\n",
    "                    self.accuracy_score = make_confusion_matrix(Yt, Yp)\n",
    "                    #\n",
    "                #####\n",
    "                #\n",
    "                # wheigts saving\n",
    "                self.weights = pd.DataFrame(model.coef_[0], self.columns).T\n",
    "                #\n",
    "                #\n",
    "            elif self.model_name == model_name[\"th_nls\"]:\n",
    "                # feedback\n",
    "                print(\"'\" + model_name[\"th_nls\"] + \"' chosen\")\n",
    "                #\n",
    "                # making df\n",
    "                df         = X.copy()\n",
    "                df[\"ISCE\"] = Y\n",
    "                #\n",
    "                #th_analysis_for_b_classification\n",
    "                model = return_th_nls_function()\n",
    "                #\n",
    "                # running\n",
    "                model = model(df)\n",
    "                del df\n",
    "                #\n",
    "                #\n",
    "            elif self.model_name == model_name[\"gini\"]:\n",
    "                # feedback\n",
    "                print(\"'\" + model_name[\"gini\"] + \"' chosen\")\n",
    "                #\n",
    "                # gini impurities\n",
    "                model = Feature_Importances(X, Y)\n",
    "                #\n",
    "                # running\n",
    "                model.evaluate_gini()\n",
    "                #\n",
    "                #\n",
    "            else:\n",
    "                # feedback\n",
    "                print(\"'self.model_name' is INvalid\")\n",
    "                #\n",
    "            #####\n",
    "            #\n",
    "            self.model     = model\n",
    "            self.if_runned = True\n",
    "            del self.X, self.Y\n",
    "            del self.X_train, self.X_test\n",
    "            del self.Y_train, self.Y_test\n",
    "        else:\n",
    "            # feedback\n",
    "            print(\"model runned yet\")\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabc920-1967-4c6d-8006-606716997355",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Random Forest model\n",
    "The Random Forest algorithm is designed to build a forest of different decision trees. Each tree is a diagram used to correctly classify an entry (row) of the dataset performing a chain of binary choices. At the end of the choice chain the entry is classified as belonging or not to the common envelope kind.\n",
    "\n",
    "In order to choice the best intermediate node among all features a numerical evaluation is computed, called the Gini impurity (more on the specific class). Such number is computed among different features in order to chose:\n",
    "1. the feature with which starting the tree (the root);\n",
    "2. the threshold (of the chosen feature) to use for the binary choice (more or less the threshold?).\n",
    "After the root node evaluation the set is splitted according to the binary choice of the root node and then another Gini impurities analysis is performed separately in each of the two subset in order to find the following two nodes.\n",
    "\n",
    "When a node cannot be splitted anymore if not in the remaining binary choice of interest then the leaf node rapresents if the system is or not evolving through a common envelope, depending on the other physical parameters.\n",
    "\n",
    "In order to prevent the risk of overfitting, instead of build a single decision tree, the algorithm finds 'n_estimators' trees (100 in the function) randomly choosing among both different entries and different features, and then the final binary classification choice is made weighting among all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b98503-71c3-489c-a9ad-0356972178cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "def return_RF_model(crit, featrs):\n",
    "    model = RandomForestClassifier(n_estimators             = 100,     # The number of trees in the forest;\n",
    "                                   criterion                = crit,    # The function to measure the quality of a split\n",
    "                                                                       # {\"gini\", \"entropy\", \"log_loss\"};\n",
    "                                   max_depth                = None,    # The maximum depth of the tree;\n",
    "                                   min_samples_split        = 2,       # The minimum number of samples required to split an internal node;\n",
    "                                   min_samples_leaf         = 1,       # The minimum number of samples required to be at a leaf node;\n",
    "                                   min_weight_fraction_leaf = 0.0,     # The minimum weighted fraction of the sum total of weights\n",
    "                                                                       # (of all the input samples) required to be at a leaf node;\n",
    "                                   max_features             = featrs,  # The number of features to consider when looking for the best split\n",
    "                                                                       # {\"sqrt\", \"log2\", None} | int\n",
    "                                                                       # max_features = sqrt(n_features) | log2(n_features) | n_features;\n",
    "                                   max_leaf_nodes           = None,    # If None then unlimited number of leaf nodes;\n",
    "                                   min_impurity_decrease    = 0.0,     # If this split induces a decrease of the impurity >=\n",
    "                                                                       # then that node will be splitted;\n",
    "                                   bootstrap              = True,      # If False, the whole dataset is used to build each tree;\n",
    "                                   oob_score              = False,     # If to use out-of-bag samples to estimate the generalization score;\n",
    "                                   n_jobs                 = -1,        # The number of jobs to run in parallel\n",
    "                                                                       # \"-1\" means using all processors;\n",
    "                                   random_state           = RS_choice, # Used for reproducibility;\n",
    "                                   verbose                = 2,         # Controls the verbosity when fitting and predicting;\n",
    "                                   warm_start             = False,     # if True reuse solution of previous call to add more estimators;\n",
    "                                   class_weight           = None,      # dict of Weights associated with features;\n",
    "                                   ccp_alpha              = 0.0,       # The subtree with the largest cost complexity that is smaller\n",
    "                                                                       # than this will be chosen;\n",
    "                                   max_samples            = None,      # If bootstrap is True, it is the number of samples to draw from X;\n",
    "                                   monotonic_cst          = None)      # If is None then no constraints are applied;\n",
    "    return model\n",
    "    #\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a0d19-27ae-465e-bc50-2f0ec640edae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Support Vector Machines\n",
    "Support Vector Machines is a model in which a binary classifier is runned in order to classify a set of points. The classifier works trying to separate a set living in a hyperspace into two sub-set using an hyperplane.\n",
    "\n",
    "The splitting mechanism does not work in all situation, depending on the nature of the set; so a way to generalize this process is to artificially increase the dimension of dataset, adding new degrees of freedom dependent on the previos ones.\n",
    "\n",
    "Increasing dimensionality is the way thanks which the classifier is able to separate with an hyperplane a set of points belonging to two distinct cathegory. How many dimension to add and which dipendence to use in order to optimize the efficiency of the computation is dependent on specific cases.\n",
    "\n",
    "Here we use 'linear' kernel which means using an hyperplane to separate points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62647e-46ca-4061-abe3-fbfb60a2d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector machines\n",
    "def return_SVM_model(K = \"linear\", D = 3):\n",
    "    model = SVC(C                       = 1.0,         # Regularization parameter: the strength is inversely proportional to C;\n",
    "                kernel                  = K,           # The kernel type to be used\n",
    "                                                       # {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'};\n",
    "                degree                  = D,           # Degree of the polynomial kernel function ('poly');\n",
    "                gamma                   = \"scale\",     # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "                                                       # {'scale', 'auto'};\n",
    "                coef0                   = 0.0,         # Independent term in kernel function; significant in 'poly' and 'sigmoid';\n",
    "                shrinking               = True,        # Whether to use the shrinking heuristic;\n",
    "                probability             = False,       # Whether to enable probability estimates;\n",
    "                tol                     = 0.001,       # Tolerance for stopping criterion;\n",
    "                cache_size              = 200,         # The size of the kernel cache (in MB);\n",
    "                class_weight            = None,        # Set the parameter C of class i to class_weight[i] * C;\n",
    "                verbose                 = True,        # Controls the verbosity when fitting and predicting;\n",
    "                max_iter                = -1,          # Hard limit on iterations within solver, or -1 for no limit;\n",
    "                decision_function_shape = \"ovr\",       # The parameter is ignored for binary classification; \n",
    "                break_ties              = False,       # If true and number of classes > 2, will break ties (at high computational cost);\n",
    "                random_state            = RS_choice)   # Used for reproducibility;\n",
    "    return model\n",
    "    #\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e5be7-30e9-49af-aaf7-9de883a0b264",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gini Impurities\n",
    "The 'Feature_Importances' class is aimed to evaluate the Gini impurities of each feature. For a matter of computational power the evaluation is performed among one part randomly chosen over tens.\n",
    "\n",
    "First of all values of the choosen feature are sorted in the ascending order. For each pair of adjacent values the average is computed, then the following values are evaluated for each average:\n",
    "1. the number of '**False**' among the entries with the chosen feature **less** than the average;\n",
    "2. the number of '**True**' among the entries with the chosen feature **less** than the average;\n",
    "3. the number of '**False**' among the entries with the chosen feature **more** than the average;\n",
    "2. the number of '**True**' among the entries with the chosen feature **more** than the average.\n",
    "\n",
    "Points 1. and 2. are summed togheter (obtaining '__D_m__') and used to normalized each of the two points; results are then squared and both subtracted from one (obtaining '__leaf_m__'); the computation is performed again for points 3. and 4. (obtaining '__D_M__' and '__leaf_M__'). '__leaf_m__' and '__leaf_M__' are then used to weight a weighted sum in which the two terms are: __D_m / (D_m + D_M)__ and __D_M / (D_m + D_M)__.\n",
    "\n",
    "When Gini impurity is evaluated for each pair of values then the minimum is chosen and the corresponding average became the threshold of the node in the tree building process; this class is not aimed to evaluate trees, just the minimum and the corresponding threshold of each feature.\n",
    "\n",
    "The less is the Gini impurity the more will be the role played by the corresponding feature in determining the behavior of the physical system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0416e-afb6-472d-8eb6-9278ab17df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Importances:\n",
    "    def __init__(self, x, y):\n",
    "        if not debugging:\n",
    "            # select 1/10 of the dataset\n",
    "            x[\"CHOICE\"] = np.random.randint(0, 10, x.shape[0])\n",
    "            x           = x[x[\"CHOICE\"] == 0]\n",
    "            x           = x[x.columns[:-1]]\n",
    "            y           = y.iloc[x.index]\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        self.x          = x\n",
    "        self.y          = y\n",
    "        self.x_columns  = list(x.columns)\n",
    "        self.index      = x.index\n",
    "        self.n_rows     = x.shape[0]\n",
    "        self.n_columns  = x.shape[1]\n",
    "        self.impurities = None\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def cellular_gini(self, index, column):\n",
    "        if ((index % 5000) == 0): print(column, \"progress: \", int((index / self.n_rows) * 1e6) / 1e4, \"%\")\n",
    "        root_node = pd.DataFrame(np.zeros((3, 2)), index = [\"True\", \"False\", \"Sums\"], columns = [\"<\", \">\"])\n",
    "        #\n",
    "        root_node.loc[\"True\",  \"<\"] =    self.y[ :index].astype(int).sum()\n",
    "        root_node.loc[\"False\", \"<\"] = (~ self.y[ :index]).astype(int).sum()\n",
    "        root_node.loc[\"True\",  \">\"] =    self.y[index: ].astype(int).sum()\n",
    "        root_node.loc[\"False\", \">\"] = (~ self.y[index: ]).astype(int).sum()\n",
    "        root_node.loc[\"Sums\",  \"<\"] = root_node.sum(axis = 0)[\"<\"]\n",
    "        root_node.loc[\"Sums\",  \">\"] = root_node.sum(axis = 0)[\">\"]\n",
    "        #\n",
    "        D_m = root_node.loc[\"Sums\",  \"<\"]\n",
    "        D_M = root_node.loc[\"Sums\",  \">\"]\n",
    "        D   = root_node.sum(axis = 1)[\"Sums\"]\n",
    "        #\n",
    "        leaf_m   = 1 - ((root_node.loc[\"True\",  \"<\"] / D_m) ** 2) - ((root_node.loc[\"False\",  \"<\"] / D_m) ** 2)\n",
    "        leaf_M   = 1 - ((root_node.loc[\"True\",  \">\"] / D_M) ** 2) - ((root_node.loc[\"False\",  \">\"] / D_M) ** 2)\n",
    "        impurity = ((D_m / D) * leaf_m) + ((D_M / D) * leaf_M)\n",
    "        #\n",
    "        return impurity\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def evaluate_column(self, column):\n",
    "        self.x = self.x.sort_values(by = column, ascending = True)\n",
    "        self.y = self.y.loc[self.x.index]\n",
    "        #\n",
    "        gini_impurities = [self.cellular_gini(index, column) for index in list(range(self.n_rows))[1:]]\n",
    "        MIN             = min(gini_impurities)\n",
    "        index           = gini_impurities.index(MIN)\n",
    "        th              = (self.x[column].iloc[index] + self.x[column].iloc[index + 1]) / 2\n",
    "        #\n",
    "        self.x = self.x.loc[self.index]\n",
    "        self.y = self.y.loc[self.index]\n",
    "        #\n",
    "        impurity        = pd.DataFrame([[MIN, th]], index = [column], columns = [\"gini\", \"th\"])\n",
    "        self.impurities = pd.concat([self.impurities, impurity], ignore_index = False)\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    def evaluate_gini(self):\n",
    "        _ = [self.evaluate_column(column) for column in self.x_columns]\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "#####\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4a4f1-aefc-414e-824b-11a55924312d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Threshold Analysis for Binary Classification\n",
    "The following function has a behavior similar to the Gini impurities evaluation ('.evaluate_gini()' method). The implementation is different in the way of finding the threshold: instead of looking for all adjacent values, performs a binary search among values.\n",
    "\n",
    "Also the computed value is different: instead of the Gini Impurities the algorithm explicitly compute the fraction of prominent binary guesses; for each potential threshold the function evaluates how many 'True' and 'False' belongs to each part of the interval dived by the threshold. The following evaluation try to maximize heterogeneity in boolean values distribution among intervals moving the potential threshold.\n",
    "\n",
    "The result of the 'th_analysis_for_b_classification()' function is the fraction of 'True' or 'False' in each interval and the corresponding threshold; the procedure is runned in a indipendent way for each ALPHA and METAL parameter combination.\n",
    "\n",
    "The chosen feature with the smaller interval of maximizing thresholds for all ALPHA and METAL combinations shall be considered as the most effective, exhibiting both high predictability power and stability among all parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835194ec-f40e-4056-84c6-fdef82ba1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A threshold optimization algorithm for binary classification tasks\n",
    "# 'threshold_analysis_for_binary_classification'\n",
    "def return_th_nls_function():\n",
    "    def th_analysis_for_b_classification(df_in):\n",
    "        features = list(df_in.columns)[2:-1]\n",
    "        #\n",
    "        #\n",
    "        def find_scale_in_base_2(range):\n",
    "            scale = 0\n",
    "            while (range / (2 ** scale)) > 1: scale += 1\n",
    "            return scale + 2\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        #\n",
    "        def find_new_range(MIN, MAX, TH):\n",
    "            average = (MIN + MAX) / 2\n",
    "            if TH > average:\n",
    "                new_MIN = average\n",
    "                new_MAX = MAX\n",
    "            else:\n",
    "                new_MIN = MIN\n",
    "                new_MAX = average\n",
    "                #\n",
    "            #####\n",
    "            #\n",
    "            return new_MIN, new_MAX\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        #\n",
    "        def evaluate_th(df, ft, TH, is_major, best, th_best):\n",
    "            df[\"BOOL\"]  = (df[ft] > TH) if is_major else (df[ft] < TH)\n",
    "            df[\"IP\"]    = ((df[\"BOOL\"] & df[\"ISCE\"]) | ((~ df[\"BOOL\"]) & (~ df[\"ISCE\"]))).astype(int)\n",
    "            esit        = (df[\"IP\"].sum() / df[\"IP\"].size)\n",
    "            if best     < esit:\n",
    "                best    = esit\n",
    "                th_best = TH\n",
    "                #\n",
    "            #####\n",
    "            #\n",
    "            return th_best, best\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        #\n",
    "        def find_best_th(df, ths, ft):\n",
    "            best_1, th_best_1 = 0, 0\n",
    "            best_2, th_best_2 = 0, 0\n",
    "            best,   th_max    = 0, 0\n",
    "            is_major          = True\n",
    "            #\n",
    "            for TH in ths:\n",
    "                th_best_1, best_1 = evaluate_th(df, ft, TH, True,  best_1, th_best_1)\n",
    "                th_best_2, best_2 = evaluate_th(df, ft, TH, False, best_2, th_best_2)\n",
    "                #\n",
    "            #####\n",
    "            #\n",
    "            if best_1 > best_2:\n",
    "                best, th_max = best_1, th_best_1\n",
    "            else:\n",
    "                best, th_max = best_2, th_best_2\n",
    "                is_major     = False\n",
    "                #\n",
    "            #####\n",
    "            #\n",
    "            return best, th_max, is_major\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        #\n",
    "        def find_best_points(df_tmp, ft):\n",
    "            result = []\n",
    "            for i in ALPHA:\n",
    "                for j in METAL:\n",
    "                    df       = df_tmp[(df_tmp[\"ALPHA\"] == i) & (df_tmp[\"METAL\"] == j)][[ft, \"ISCE\"]]\n",
    "                    progress = ((int((((len(METAL) * ALPHA.index(i)) + METAL.index(j)) / (len(ALPHA) * len(METAL))) * 1000)) / 10)\n",
    "                    print(\"feature: \", ft, \" progress: \", progress, \"%\")\n",
    "                    #\n",
    "                    MIN      = df[ft].min()\n",
    "                    MAX      = df[ft].max()\n",
    "                    scale    = find_scale_in_base_2(MAX / MIN)\n",
    "                    ths      = np.linspace(MIN, MAX, 100)\n",
    "                    #\n",
    "                    best, TH, is_major      = find_best_th(df, ths, ft)\n",
    "                    for w in range(scale):\n",
    "                        number_of_points    = int(50 + (50 * (w/scale)))\n",
    "                        MIN, MAX            = find_new_range(MIN, MAX, TH)\n",
    "                        ths                 = np.linspace(MIN, MAX, number_of_points)\n",
    "                        best, TH, is_major  = find_best_th(df_tmp, ths, ft)\n",
    "                        #\n",
    "                    #####\n",
    "                    #\n",
    "                    result_row = [i, j, TH, is_major, best]\n",
    "                    result    += [result_row]\n",
    "                    del df\n",
    "                    #\n",
    "                #####\n",
    "                #\n",
    "            #####\n",
    "            #\n",
    "            result            = pd.DataFrame(np.array(result))\n",
    "            result[\"feature\"] = ft\n",
    "            #\n",
    "            return result\n",
    "            #\n",
    "        #####\n",
    "        #\n",
    "        result     = [find_best_points(df_in, feature) for feature in features]\n",
    "        df         = pd.concat(result, ignore_index = True)\n",
    "        df.columns = [\"ALPHA\", \"METAL\", \"TH\", \"is_major\", \"PREDIC. [%]\", \"feature\"]\n",
    "        #\n",
    "        return df\n",
    "        #\n",
    "    #####\n",
    "    #\n",
    "    return th_analysis_for_b_classification\n",
    "    #\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c68964e-21dc-409d-89bb-066613a0c308",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Working data, paths and models\n",
    "The following code is aimed to create instances of files and models and perform computations. If a computation is already done (meaning the corresponding file exists in the right place) then the model is simply loaded from the file; otherwise it will be saved after the running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32731d-8674-4e68-a5e7-94a120643360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "#\n",
    "X0, Y0, X1, Y1 = reading_all_datasets()\n",
    "if debugging:\n",
    "    X0N, Y0N, X1N, Y1N = pd.DataFrame([]), pd.DataFrame([]), pd.DataFrame([]), []\n",
    "    for A in ALPHA:\n",
    "        for M in METAL:\n",
    "            X0N = pd.concat([X0N, X0[(X0[\"ALPHA\"] == A) & (X0[\"METAL\"] == M)].iloc[0:2]], ignore_index = False)\n",
    "            X1N = pd.concat([X1N, X1[(X1[\"ALPHA\"] == A) & (X1[\"METAL\"] == M)].iloc[0:2]], ignore_index = False)\n",
    "            #\n",
    "    #########\n",
    "    X0 = X0N\n",
    "    X1 = X1N\n",
    "    Y0 = Y0.iloc[X0N.index]\n",
    "    Y1 = Y1.iloc[X1N.index]\n",
    "    del X0N, X1N\n",
    "    #\n",
    "#####\n",
    "#\n",
    "# describing\n",
    "# X0.describe().iloc[[1, 2, 3, 7]].T\n",
    "# X1.describe().iloc[[1, 2, 3, 7]].T\n",
    "#\n",
    "#\n",
    "#\n",
    "# paths\n",
    "#\n",
    "#RF_all_ORIGIN             = DATA(dataset_version[\"ORIGIN\"], dir[\"RF_all\"])\n",
    "#RF_all_DER_1              = DATA(dataset_version[\"DER_1\"],  dir[\"RF_all\"])\n",
    "#RF_all_DER_2              = DATA(dataset_version[\"DER_2\"],  dir[\"RF_all\"])\n",
    "#\n",
    "RF_partial_ORIGIN         = DATA(dataset_version[\"ORIGIN\"], dir[\"RF_partial\"])           # output of the 'Random Forest Classifier'\n",
    "RF_partial_DER_1          = DATA(dataset_version[\"DER_1\"],  dir[\"RF_partial\"])           # algorithm; for both datasets\n",
    "#\n",
    "RF_all_th_analysis_ORIGIN = DATA(dataset_version[\"ORIGIN\"], dir[\"RF_all_th_analysis\"])   # output of the 'th_analysis_for_b_classification'\n",
    "RF_all_th_analysis_DER_1  = DATA(dataset_version[\"DER_1\"],  dir[\"RF_all_th_analysis\"])   # function; for both datasets\n",
    "#\n",
    "RF_all_impurities_ORIGIN  = DATA(dataset_version[\"ORIGIN\"], dir[\"RF_all_impurities\"])    # output of the '.evaluate_gini' method;\n",
    "RF_all_impurities_DER_1   = DATA(dataset_version[\"DER_1\"],  dir[\"RF_all_impurities\"])    # for bath datasets\n",
    "#\n",
    "#SVM_all_ORIGIN            = DATA(dataset_version[\"ORIGIN\"], dir[\"SVM_all\"])\n",
    "#SVM_all_DER_1             = DATA(dataset_version[\"DER_1\"],  dir[\"SVM_all\"])\n",
    "#\n",
    "SVM_partial_ORIGIN        = DATA(dataset_version[\"ORIGIN\"], dir[\"SVM_partial\"])          # output of the 'Support Vector Machines'\n",
    "SVM_partial_DER_1         = DATA(dataset_version[\"DER_1\"],  dir[\"SVM_partial\"])          # algorithm; for both datasets\n",
    "#\n",
    "#\n",
    "#\n",
    "# create models\n",
    "#\n",
    "MOD_RF_partial_ORIGIN         = CREATE(RF_partial_ORIGIN,         MODEL(model_name[\"RF\"],     X0, Y0, all = False))\n",
    "MOD_RF_partial_DER_1          = CREATE(RF_partial_DER_1,          MODEL(model_name[\"RF\"],     X1, Y1, all = False))\n",
    "#\n",
    "MOD_RF_all_th_analysis_ORIGIN = CREATE(RF_all_th_analysis_ORIGIN, MODEL(model_name[\"th_nls\"], X0, Y0, all = True))\n",
    "MOD_RF_all_th_analysis_DER_1  = CREATE(RF_all_th_analysis_DER_1,  MODEL(model_name[\"th_nls\"], X1, Y1, all = True))\n",
    "#\n",
    "MOD_RF_all_impurities_ORIGIN  = CREATE(RF_all_impurities_ORIGIN,  MODEL(model_name[\"gini\"],   X0, Y0, all = True))\n",
    "MOD_RF_all_impurities_DER_1   = CREATE(RF_all_impurities_DER_1,   MODEL(model_name[\"gini\"],   X1, Y1, all = True))\n",
    "#\n",
    "MOD_SVM_partial_ORIGIN        = CREATE(SVM_partial_ORIGIN,        MODEL(model_name[\"SVM\"],    X0, Y0, all = False))\n",
    "MOD_SVM_partial_DER_1         = CREATE(SVM_partial_DER_1,         MODEL(model_name[\"SVM\"],    X1, Y1, all = False))\n",
    "#\n",
    "#\n",
    "#\n",
    "# run if not runned yet\n",
    "#\n",
    "MOD_RF_partial_ORIGIN.run()\n",
    "MOD_RF_partial_DER_1.run()\n",
    "#\n",
    "MOD_RF_all_th_analysis_ORIGIN.run()\n",
    "MOD_RF_all_th_analysis_DER_1.run()\n",
    "#\n",
    "MOD_RF_all_impurities_ORIGIN.run()\n",
    "MOD_RF_all_impurities_DER_1.run()\n",
    "#\n",
    "MOD_SVM_partial_ORIGIN.run()\n",
    "MOD_SVM_partial_DER_1.run()\n",
    "#\n",
    "#\n",
    "#\n",
    "# save results\n",
    "#\n",
    "RF_partial_ORIGIN.save(MOD_RF_partial_ORIGIN)\n",
    "RF_partial_DER_1.save(MOD_RF_partial_DER_1)\n",
    "#\n",
    "RF_all_th_analysis_ORIGIN.save(MOD_RF_all_th_analysis_ORIGIN)\n",
    "RF_all_th_analysis_DER_1.save(MOD_RF_all_th_analysis_DER_1)\n",
    "#\n",
    "RF_all_impurities_ORIGIN.save(MOD_RF_all_impurities_ORIGIN)\n",
    "RF_all_impurities_DER_1.save(MOD_RF_all_impurities_DER_1)\n",
    "#\n",
    "SVM_partial_ORIGIN.save(MOD_SVM_partial_ORIGIN)\n",
    "SVM_partial_DER_1.save(MOD_SVM_partial_DER_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ecec37-ddcb-4a26-b5a2-4c6900d65a89",
   "metadata": {},
   "source": [
    "# Results loader\n",
    "In this last part evaluated models are loaded and results are explicitly shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91032e00-3640-4597-aff4-fc35a6bf3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOD_RF_partial_ORIGIN.weights\n",
    "MOD_RF_partial_ORIGIN.accuracy_score\n",
    "MOD_RF_partial_DER_1.weights\n",
    "MOD_RF_partial_DER_1.accuracy_score\n",
    "#\n",
    "MOD_RF_all_th_analysis_ORIGIN.model\n",
    "MOD_RF_all_th_analysis_DER_1.model\n",
    "#\n",
    "MOD_RF_all_impurities_ORIGIN.model\n",
    "MOD_RF_all_impurities_DER_1.model\n",
    "#\n",
    "MOD_SVM_partial_ORIGIN.weights\n",
    "MOD_SVM_partial_ORIGIN.accuracy_score\n",
    "MOD_SVM_partial_DER_1.weights\n",
    "MOD_SVM_partial_DER_1.accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56706ab-9ead-4ebf-90c8-f443433575a0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Our Analysis has shown a predictable behavior: the physical system does not exibit a linear dependence from parameters; this fact is evident in the failed attempt of the linear regression analysis to provide consistent results. Indeed undergoing stellar physics is deeply nonlinear: stellar dynamics is impacted by thresholds in which the behavior drastically changes.\n",
    "\n",
    "Instead both RF and SVM, algorithms usefull in binary classification, have provided consistent results, showing that stellar masses seems to play a mojor role in impacting the mass transfer among each others. Following also semi major axes seems to play an important role. Even independent confirmations came in favour of this evidence, as the two algorithms implemented for Threshold Analysis and Gini Impurities computations.\n",
    "\n",
    "Then an hypothesis of combined features is formulated among those that make physical sense, taking care to chose most prominent features. The number of derived features and the specific combinations are chose in such a way to preserve all the information caming from the masses and the semi-major axis, without adding any kind of redundance (the same amount of derived features with respect to the original ones used to deriving).\n",
    "\n",
    "Among derived features the ration between stellar masses (the little one divided the big one) seems to play the mojor role in the binary dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd67a1d-5acb-436e-8f3d-3446c3d64cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
